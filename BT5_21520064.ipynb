{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YczE55EXiYx9",
        "outputId": "d21cc2dc-d867-4337-967b-46a9ec9c4a50"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhSyhfEy4XSD"
      },
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHf1dAVKAcZm"
      },
      "source": [
        "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-6usoQHAmqh",
        "outputId": "330bcd6d-9ed7-4a3a-a519-bdf1a73e72ec"
      },
      "source": [
        "env.P[0][3] # Transition model"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 1, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh7Su0h0AqQz",
        "outputId": "a4d49217-8830-4d0b-ec5f-18f4da4464c7"
      },
      "source": [
        "env.observation_space.n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ68w5bpBScC",
        "outputId": "37c4eb84-73e9-4cff-eaa5-dd1681e6ca49"
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWLnvY7VBvIZ"
      },
      "source": [
        "def play(env, policy, render=False):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            print(env.render())\n",
        "            time.sleep(0.5)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcuDDx6rC5YE",
        "outputId": "5d2a1fe1-36b6-457e-d34e-a1cc9299f7d8"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play(env, policy_0)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play(env, policy_0, True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ1CJNPhDGPA",
        "outputId": "c33ce212-1996-4091-a3df-b82c77501266"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 77)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdyjjtGZC9NX",
        "outputId": "e22b2fc2-e9d9-44ce-fc9b-69847ae4ee08"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "play(env, policy_1, True)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt0VhyMuDasc",
        "outputId": "942c55b2-f493-4903-9d67-df7125a047b0"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play(env, policy_2, True)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp6qhRFJDxWR",
        "outputId": "1b1bf483-5be2-4ff8-ca9b-6e0f5e6ecd2d"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play(env, policy_3, True)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 26)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU8Q1qMxD6Po"
      },
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, policy)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G427z17PEmjQ",
        "outputId": "8549530e-e090-4c78-d992-ea0e7b4fbeb2"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play_multiple_times(env, policy_0, 1000)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 0/1000\n",
            "Average number of steps: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1bkhaFdDmj_",
        "outputId": "b8679657-2933-4b39-8974-c57fa9297e98"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "play_multiple_times(env, policy_1, 1000)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 57/1000\n",
            "Average number of steps: 12.421052631578947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZYhsb_VEtuR",
        "outputId": "56946864-129a-4122-a0db-f08da1645d44"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play_multiple_times(env, policy_2, 1000)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 101/1000\n",
            "Average number of steps: 16.475247524752476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvvHdMesEzTH",
        "outputId": "1d6e4334-d16d-4ed6-f2ed-09a6c8d782e4"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play_multiple_times(env, policy_3, 1000)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 770/1000\n",
            "Average number of steps: 41.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSomNpxJE5lP"
      },
      "source": [
        "def policy_evaluation(env, policy, max_iters=500, gamma=0.9):\n",
        "    # Initialize the values of all states to be 0\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Update the value of each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "\n",
        "            # Compute the q-value of the action\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value # update v-value\n",
        "\n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            # print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "\n",
        "    return v_values"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7IhqEOgGkQX",
        "outputId": "52ee11ee-f5d7-4aa6-f8e4-a44763f6ef8f"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "v_values_0 = policy_evaluation(env, policy_0)\n",
        "print(v_values_0)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 0-th iteration.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMjJKI3GGrsN",
        "outputId": "d1c09ef1-8ec4-4581-9917-52526cfd6e80"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "v_values_1 = policy_evaluation(env, policy_1)\n",
        "print(v_values_1)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 48-th iteration.\n",
            "[0.01904157 0.01519815 0.03161906 0.02371389 0.02538879 0.\n",
            " 0.06648515 0.         0.05924054 0.13822794 0.18999823 0.\n",
            " 0.         0.21152109 0.56684236 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-26M77nEfcV",
        "outputId": "0e1c8ebd-2f2d-461c-bece-97e2401f4caa"
      },
      "source": [
        "np.all(v_values_1 >= v_values_0)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l49O1N8QG0S2",
        "outputId": "e13f2a1b-97dd-4976-f5d7-9a7e0cb0e8cb"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "v_values_2 = policy_evaluation(env, policy_2)\n",
        "print(v_values_2)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 53-th iteration.\n",
            "[0.02889625 0.01951972 0.03616977 0.0271268  0.04790519 0.\n",
            " 0.07391985 0.         0.08288277 0.19339319 0.21022995 0.\n",
            " 0.         0.35153135 0.62684674 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22pRvreGE3Yt",
        "outputId": "f250ac04-cc1b-4ad9-8998-0c6aa428d9af"
      },
      "source": [
        "np.all(v_values_2 >= v_values_1)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTYYFq6BEXDd",
        "outputId": "13d1976f-d2f5-453d-f7ea-ebed555d6ca1"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "v_values_3 = policy_evaluation(env, policy_3)\n",
        "print(v_values_3)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 80-th iteration.\n",
            "[0.06888666 0.06141097 0.07440714 0.05580443 0.09185068 0.\n",
            " 0.11220679 0.         0.14543323 0.24749485 0.29961611 0.\n",
            " 0.         0.37993438 0.63901935 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcEfU3NYE7xN",
        "outputId": "ff5ae40b-6fde-44fc-9b2d-826ad739c81c"
      },
      "source": [
        "np.all(v_values_3 >= v_values_2)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh4akjMSHJBF"
      },
      "source": [
        "def value_iteration(env, max_iters=500, gamma=0.9):\n",
        "    # initialize\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "    step = 0\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # update the v-value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "\n",
        "            # compute the q-value for each action that we can perform at the state\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "                q_values.append(q_value)\n",
        "\n",
        "            # select the max q-values\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "\n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            # print(f'Converged at {i}-th iteration.')\n",
        "            step = i\n",
        "            break\n",
        "\n",
        "    return v_values, step"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8xAljw7VuMP",
        "outputId": "d2a04412-ba75-4745-b1c1-bfcd0f0e58dd"
      },
      "source": [
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 79-th iteration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7g9VA3lV2WW",
        "outputId": "2b01c9d0-303b-4f53-85d8-4e96cc279c40"
      },
      "source": [
        "optimal_v_values"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.06888615, 0.06141054, 0.07440682, 0.05580409, 0.09185022,\n",
              "       0.        , 0.11220663, 0.        , 0.14543286, 0.2474946 ,\n",
              "       0.29961593, 0.        , 0.        , 0.3799342 , 0.63901926,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb0an7gaV39e"
      },
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    # initialize\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int32)\n",
        "\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # loop through each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "\n",
        "            q_values.append(q_value)\n",
        "\n",
        "        # select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "\n",
        "    return policy"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TGCF4G7XErH"
      },
      "source": [
        "optimal_policy = policy_extraction(env, optimal_v_values, gamma=0.9)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkHYtfm4qikV",
        "outputId": "3eb818ae-5ef1-4859-8191-a5644baf5b09"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play(env, optimal_policy, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ww12Uh5qCUb",
        "outputId": "809969e9-be76-4728-9373-bf20b2f2e00f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 34)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-m4ZqWZXKqG",
        "outputId": "098bb363-e6f9-4409-86a7-5944369f3a48"
      },
      "source": [
        "play_multiple_times(env, optimal_policy, 1000)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 743/1000\n",
            "Average number of steps: 43.427994616419916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My code"
      ],
      "metadata": {
        "id": "LXWCvTH_Pro3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, max_iters=500, gamma=0.9):\n",
        "    # Initialize the values of all states to be 0\n",
        "    pi_values = np.zeros(env.observation_space.n)\n",
        "    step = 0\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_pi_values = np.copy(pi_values)\n",
        "\n",
        "        # Policy evaluation\n",
        "        v_values = policy_evaluation(env, pi_values)\n",
        "\n",
        "        # Polucy improvement\n",
        "        pi_values = policy_extraction(env, v_values)\n",
        "\n",
        "        # Check convergence\n",
        "        if np.array_equal(pi_values, prev_pi_values):\n",
        "            step = i\n",
        "            break\n",
        "\n",
        "    return pi_values, step"
      ],
      "metadata": {
        "id": "Ik7qS1xfR7K_"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def myValueIteration(env, n_iter=50):\n",
        "    runTimes = []\n",
        "    steps = []\n",
        "    for i in range(n_iter):\n",
        "        start = time.time()\n",
        "\n",
        "        # Value iteration\n",
        "        optimal_v_values, step = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "\n",
        "        # Policy Extraction\n",
        "        optimal_pi_values = policy_extraction(env, optimal_v_values, gamma=0.9)\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "        runTimes.append(end - start)\n",
        "        steps.append(step)\n",
        "\n",
        "    return runTimes, steps"
      ],
      "metadata": {
        "id": "tQsEoFkkPpfV"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def myPolicyIteration(env, n_iter=50):\n",
        "    runTimes = []\n",
        "    steps = []\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        start = time.time()\n",
        "\n",
        "        # Policy iteration\n",
        "        optimal_pi_values, step = policy_iteration(env, max_iters=500, gamma=0.9)\n",
        "\n",
        "        end = time.time()\n",
        "\n",
        "        runTimes.append(end - start)\n",
        "        steps.append(step)\n",
        "\n",
        "    return runTimes, steps"
      ],
      "metadata": {
        "id": "YOQ7Hs4DqX2T"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FrozenLake-v1"
      ],
      "metadata": {
        "id": "qePMjn_nUJ92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment\n",
        "env = gym.make('FrozenLake-v1')"
      ],
      "metadata": {
        "id": "XIjnRS4zUMDS"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value iteration\n",
        "frozenLake_value_times, frozenLake_value_steps = myValueIteration(env)"
      ],
      "metadata": {
        "id": "3Yg7yayoUy0E"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy iteration\n",
        "frozenLake_policy_times, frozenLake_policy_steps = myPolicyIteration(env)"
      ],
      "metadata": {
        "id": "tvtvswkHWiwk"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average runtime of Value Iteration: {np.average(frozenLake_value_times)} sec\")\n",
        "print(f\"Average runtime of Policy Iteration: {np.average(frozenLake_policy_times)} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2OJekr3Xduo",
        "outputId": "db86e518-2e10-47f3-de45-10f57aad0630"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average runtime of Value Iteration: 0.11993456840515136 sec\n",
            "Average runtime of Policy Iteration: 0.08793222427368164 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FrozenLake8x8-v1\n"
      ],
      "metadata": {
        "id": "gFUPTDGzXsOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake8x8-v1\")"
      ],
      "metadata": {
        "id": "WjvMXSPtXoCR"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value iteration\n",
        "frozenLake8x8_value_times, frozenLake8x8_value_steps = myValueIteration(env)"
      ],
      "metadata": {
        "id": "KtJQqN0aXvLv"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy iteration\n",
        "frozenLake8x8_policy_times, frozenLake8x8_policy_steps = myPolicyIteration(env)"
      ],
      "metadata": {
        "id": "Z5J2LXzzX0Fw"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average runtime of Value Iteration: {np.average(frozenLake8x8_value_times)} sec\")\n",
        "print(f\"Average runtime of Policy Iteration: {np.average(frozenLake8x8_policy_times)} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9goEgzZ0X3__",
        "outputId": "47f15c2a-be00-4729-c05c-5c5c1f287619"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average runtime of Value Iteration: 0.6339145755767822 sec\n",
            "Average runtime of Policy Iteration: 1.0489384841918945 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Taxi-v3"
      ],
      "metadata": {
        "id": "WIXJxyHhX_Li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Taxi-v3')"
      ],
      "metadata": {
        "id": "TIj_IaJHX9_R"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value iteration\n",
        "taxi_value_times, taxi_value_steps = myValueIteration(env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqmeGqTWYCUL",
        "outputId": "6dd033ea-7cd9-4c62-b0df-9024e45d5bc5"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Policy iteration\n",
        "taxi_policy_times, taxi_policy_steps = myPolicyIteration(env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8l3rI1iYGLs",
        "outputId": "cfa5c43f-a97e-4a08-dfe7-2fd941fab399"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Average runtime of Value Iteration: {np.average(taxi_value_times)} sec\")\n",
        "print(f\"Average runtime of Policy Iteration: {np.average(taxi_policy_times)} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd_wMgS3YJNI",
        "outputId": "58e0ca03-de5f-488c-d02b-94e584216416"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average runtime of Value Iteration: 5.9809559631347655 sec\n",
            "Average runtime of Policy Iteration: 12.996360387802124 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nhận xét"
      ],
      "metadata": {
        "id": "kNNK2Pv7YWI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\t\\t\\tAverage runtime of Value Iteration\\tAverage runtime of Policy Iteration\\n\")\n",
        "print(f\"FrozenLake-v1\\t\\t {np.average(frozenLake_value_times)} \\t\\t\\t {np.average(frozenLake_policy_times)}\\n\")\n",
        "print(f\"FrozenLake8x8-v1\\t {np.average(frozenLake8x8_value_times)} \\t\\t\\t {np.average(frozenLake8x8_policy_times)}\\n\")\n",
        "print(f\"Taxi-v3\\t\\t\\t {np.average(taxi_value_times)} \\t\\t\\t {np.average(taxi_policy_times)}\\n\")\n",
        "\n",
        "print(\"------------------------------------------------------------------------------------------------------------------\")\n",
        "print(\"\\t\\t\\tAverage step of Value Iteration \\t Average step of Policy Iteration\\n\")\n",
        "print(f\"FrozenLake-v1\\t\\t {np.average(frozenLake_value_steps)} \\t\\t\\t\\t\\t {np.average(frozenLake_policy_steps)}\\n\")\n",
        "print(f\"FrozenLake8x8-v1\\t {np.average(frozenLake8x8_value_steps)} \\t\\t\\t\\t\\t {np.average(frozenLake8x8_policy_steps)}\\n\")\n",
        "print(f\"Taxi-v3\\t\\t\\t {np.average(taxi_value_steps)} \\t\\t\\t\\t\\t {np.average(taxi_policy_steps)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCJetdTcYM8Z",
        "outputId": "60c80e6c-16a2-40e3-e40f-a08f5fd3060b"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\t\tAverage runtime of Value Iteration\tAverage runtime of Policy Iteration\n",
            "\n",
            "FrozenLake-v1\t\t 0.11993456840515136 \t\t\t 0.08793222427368164\n",
            "\n",
            "FrozenLake8x8-v1\t 0.6339145755767822 \t\t\t 1.0489384841918945\n",
            "\n",
            "Taxi-v3\t\t\t 5.9809559631347655 \t\t\t 12.996360387802124\n",
            "\n",
            "------------------------------------------------------------------------------------------------------------------\n",
            "\t\t\tAverage step of Value Iteration \t Average step of Policy Iteration\n",
            "\n",
            "FrozenLake-v1\t\t 79.0 \t\t\t\t\t 5.0\n",
            "\n",
            "FrozenLake8x8-v1\t 117.0 \t\t\t\t\t 9.0\n",
            "\n",
            "Taxi-v3\t\t\t 116.0 \t\t\t\t\t 16.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Với trung bình 50 lần chạy:\n",
        "- Trong các toy games 'FrozenLake-v1', 'FrozenLake8x8-v1', và 'Taxi-v3', thuật toán Value iteration đều cho ra kết quả nhanh hơn thuật toán Policy Iteration.\n",
        "Đặc biệt, trong games Taxi-v3, thuật toán Value iteration chạy nhanh hơn Policy iteration ~2 lần.\n",
        "\n",
        "- Trong các toy games ở trên, có thể thấy thuật toán Policy Iteration mất rất ít vòng lặp để hội tụ, thế nhưng lại mất rất nhiều thời gian để cho ra kết quả đó.\n",
        "Điều này cho thấy rằng với mỗi chiến lược, thuật toán Policy tốn rất nhiều thời gian để đánh giá và cải thiện chúng.\n",
        "\n",
        "- Tóm lại, ta có kết luận sau: thuật toán Value iteration mất rất nhiều bước để hội tụ, nhưng thời gian tính toán lại nhanh hơn so với thuật toán Policy Iteration.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fZJAEKNjZ6am"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}